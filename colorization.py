# -*- coding: utf-8 -*-
"""Colorization

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1IqChdHkNTkj3C5kd5hJblU_nKmwPNbF1

# Get Dataset from Google Drive
"""

from google.colab import drive
drive.mount('/content/drive/')

import os
import zipfile
import tqdm

file_name = "Multimedia_dataset.zip"
zip_path = os.path.join('/content/drive/MyDrive/Multimedia_dataset.zip')

!cp "{zip_path}" .
!unzip -q "{file_name}"
!rm "{file_name}"

import os
import zipfile
import tqdm

file_name = "colorization_test_dataset.zip"
zip_path = os.path.join('/content/drive/MyDrive/colorization_test_dataset.zip')

!cp "{zip_path}" .
!unzip -q "{file_name}"
!rm "{file_name}"

"""# ColorHintTransform and Dataset"""

import torch
from torch.autograd import Variable
from torchvision import transforms

import cv2
import random
import numpy as np

class ColorHintTransform(object):
  def __init__(self, size=256, mode="training"):
    super(ColorHintTransform, self).__init__()
    self.size = size
    self.mode = mode
    self.transform = transforms.Compose([transforms.ToTensor()])

  def bgr_to_lab(self, img):
    lab = cv2.cvtColor(img, cv2.COLOR_BGR2LAB)
    l, ab = lab[:, :, 0], lab[:, :, 1:]
    return l, ab

  def hint_mask(self, bgr, threshold=[0.95, 0.97, 0.99]):
    h, w, c = bgr.shape
    mask_threshold = random.choice(threshold)
    mask = np.random.random([h, w, 1]) > mask_threshold
    return mask

  def img_to_mask(self, mask_img):
    mask = mask_img[:, :, 0, np.newaxis] >= 255
    return mask

  def __call__(self, img, mask_img=None):
    threshold = [0.95, 0.97, 0.99]
    if (self.mode == "training") | (self.mode == "validation"):
      image = cv2.resize(img, (self.size, self.size))
      mask = self.hint_mask(image, threshold)

      hint_image = image * mask

      l, ab = self.bgr_to_lab(image)
      l_hint, ab_hint = self.bgr_to_lab(hint_image)

      return self.transform(l), self.transform(ab), self.transform(ab_hint)

    elif self.mode == "testing":
      image = cv2.resize(img, (self.size, self.size))
      hint_image = image * self.img_to_mask(mask_img)

      l, _ = self.bgr_to_lab(image)
      _, ab_hint = self.bgr_to_lab(hint_image)

      return self.transform(l), self.transform(ab_hint)

    else:
      return NotImplementedError

"""#DataLoader for Colorization Dataset"""

import torch
import torch.utils.data  as data
import os
import cv2
from google.colab.patches import cv2_imshow

class ColorHintDataset(data.Dataset):
  def __init__(self, root_path, size):
    super(ColorHintDataset, self).__init__()

    self.root_path = root_path
    self.size = size
    self.transforms = None
    self.examples = None
    self.hint = None
    self.mask = None

  def set_mode(self, mode):
    self.mode = mode
    self.transforms = ColorHintTransform(self.size, mode)
    if mode == "training":
      train_dir = os.path.join(self.root_path, "train")
      self.examples = [os.path.join(self.root_path, "train", dirs) for dirs in os.listdir(train_dir)]
    elif mode == "validation":
      val_dir = os.path.join(self.root_path, "validation")
      self.examples = [os.path.join(self.root_path, "validation", dirs) for dirs in os.listdir(val_dir)]
    elif mode == "testing":
      hint_dir = os.path.join(self.root_path, "hint")
      mask_dir = os.path.join(self.root_path, "mask")
      self.hint = [os.path.join(self.root_path, "hint", dirs) for dirs in os.listdir(hint_dir)]
      self.mask = [os.path.join(self.root_path, "mask", dirs) for dirs in os.listdir(mask_dir)]
    else:
      raise NotImplementedError

  def __len__(self):
    if self.mode != "testing":
      return len(self.examples)
    else:
      return len(self.hint)

  def __getitem__(self, idx):
    if self.mode == "testing":
      hint_file_name = self.hint[idx]
      mask_file_name = self.mask[idx]
      hint_img = cv2.imread(hint_file_name)
      mask_img = cv2.imread(mask_file_name)

      input_l, input_hint = self.transforms(hint_img, mask_img)
      sample = {"l": input_l, "hint": input_hint,
                "file_name": "image_%06d.png" % int(os.path.basename(hint_file_name).split('.')[0])}
    else:
      file_name = self.examples[idx]
      img = cv2.imread(file_name)
      l, ab, hint = self.transforms(img)
      sample = {"l": l, "ab": ab, "hint": hint}

    return sample

"""#Loading"""

import torch
import torch.utils.data  as data
import os
import cv2
from google.colab.patches import cv2_imshow
import matplotlib.pyplot as plt
from torchvision import transforms
import tqdm
from PIL import Image
import numpy as np

def tensor2im(input_image,imtype=np.uint8):
  if isinstance(input_image,torch.Tensor):
    image_tensor=input_image.data
  else:
    return input_image
  
  image_numpy=image_tensor[0].cpu().float().numpy()
  if image_numpy.shape[0]==1:
    image_numpy=np.tile(image_numpy,(3,1,1))
  image_numpy=np.clip((np.transpose(image_numpy,(1,2,0)) ),0,1)*255.0
  return image_numpy.astype(imtype)

root_path="/content/"
use_cuda=True

train_dataset=ColorHintDataset(root_path,128)
train_dataset.set_mode("training")
train_dataloader=data.DataLoader(train_dataset, batch_size=4,shuffle=True)

valid_dataset=ColorHintDataset(root_path,128)
valid_dataset.set_mode("validation")
valid_dataloader=data.DataLoader(valid_dataset,shuffle=False)


test_dataset=ColorHintDataset(root_path,128)
test_dataset.set_mode("testing")
test_dataloader=data.DataLoader(valid_dataset,shuffle=False)

"""#Model"""

import torch
import torch.nn as nn
import torch.nn.functional as F

# 3*3 conv+batch normalization+relu 2번
class DoubleConv(nn.Module):
  def __init__(self,in_channels,out_channels,mid_channels=None):
    super().__init__()
    if not mid_channels:
      mid_channels=out_channels
    self.double_conv = nn.Sequential(
        nn.Conv2d(in_channels,mid_channels,kernel_size=3,padding=1),
        nn.BatchNorm2d(mid_channels),
        nn.ReLU(inplace=True),
        nn.Conv2d(mid_channels,out_channels,kernel_size=3,padding=1),
        nn.BatchNorm2d(out_channels),
        nn.ReLU(inplace=True)
    )
  def forward(self,x):
    return self.double_conv(x)


class Down(nn.Module):
  def __init__(self,in_channels,out_channels):
     super().__init__()
     self.maxpool_conv=nn.Sequential(
         nn.MaxPool2d(2),
         DoubleConv(in_channels,out_channels)
     )
  def forward(self,x):
    return self.maxpool_conv(x)


class Up(nn.Module):
  def __init__(self,in_channels,out_channels,bilinear=True):
    super().__init__()

    # if bilinear:
    #   
    #   self.up=nn.Upsample(scale_factor=2,mode='bileanear',align_corners=True) #간단 .가중치 학습x
    #   self.conv=DoubleConv(in_channels,out_channels,inchannels//2)
    # else:
    #2x2 convolution / upsampling 할때마다 채널수가 절반으로 줄어듬
    self.up=nn.ConvTranspose2d(in_channels,in_channels//2,kernel_size=2,stride=2)
    self.conv=DoubleConv(in_channels,out_channels)
  
  def forward(self,x1,x2):
    x1=self.up(x1)
    #각 expanding step 마다 up-conv 된 feature map은 contracting path의 cropped 된 특징맵과 concat 
    x = torch.cat([x2, x1], dim=1)
    return self.conv(x)

class OutConv(nn.Module):
  def __init__(self,in_channels,out_channels):
    super(OutConv,self).__init__()
    self.conv=nn.Conv2d(in_channels,out_channels,kernel_size=1)

  def forward(self,x):
    return self.conv(x)



class Unet(nn.Module):
  def __init__(self,n_channels,n_classes,bilinear=False):
    super(Unet,self).__init__()
    self.n_channels=n_channels #3
    self.n_classes=n_classes #3
    self.bilinear=bilinear

    self.inc=DoubleConv(n_channels,64)
    self.down1=Down(64,128) #maxpooling + doubleConv
    self.down2=Down(128,256)
    self.down3=Down(256,512)
    self.down4=Down(512,1024)

    self.up1=Up(1024,512) #upsample+concat+doubleConv
    self.up2=Up(512,256)
    self.up3=Up(256,128)
    self.up4=Up(128,64)
    self.outc=OutConv(64,n_classes)

  def forward(self,x):
    x1=self.inc(x)
    x2=self.down1(x1)
    x3=self.down2(x2)
    x4=self.down3(x3)
    x5=self.down4(x4)
    x=self.up1(x5,x4)
    x=self.up2(x,x3)
    x=self.up3(x,x2)
    x=self.up4(x,x1)
    logits=self.outc(x)
    return logits

class Nested_Unet(nn.Module):
    def __init__(self, n_channels, n_classes, deep_supervision=False):
        super().__init__()
        self.deep_supervision = deep_supervision
        self.pool = nn.MaxPool2d(2, 2)
        #self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)

        self.up1=nn.ConvTranspose2d(128,64,kernel_size=2,stride=2)
        self.up2=nn.ConvTranspose2d(256,128,kernel_size=2,stride=2)
        self.up3=nn.ConvTranspose2d(512,256,kernel_size=2,stride=2)
        self.up4=nn.ConvTranspose2d(1024,512,kernel_size=2,stride=2)

        self.conv0_0 = DoubleConv(n_channels, 64)
        self.conv1_0 = DoubleConv(64, 128)
        self.conv2_0 = DoubleConv(128, 256)
        self.conv3_0 = DoubleConv(256, 512)
        self.conv4_0 = DoubleConv(512, 1024)

        self.conv0_1 = DoubleConv(64*2, 64)
        self.conv1_1 = DoubleConv(128*2, 128)
        self.conv2_1 = DoubleConv(256*2, 256 )
        self.conv3_1 = DoubleConv(512*2, 512)

        self.conv0_2 = DoubleConv(64*3,64)
        self.conv1_2 = DoubleConv(128*3, 128)
        self.conv2_2 = DoubleConv(256*3, 256)

        self.conv0_3 = DoubleConv(64*4, 64)
        self.conv1_3 = DoubleConv(128*4, 128)

        self.conv0_4 = DoubleConv(64*5, 64)

        if self.deep_supervision:
            self.final1 = nn.Conv2d(64, n_classes, kernel_size=1)
            self.final2 = nn.Conv2d(64, n_classes, kernel_size=1)
            self.final3 = nn.Conv2d(64, n_classes, kernel_size=1)
            self.final4 = nn.Conv2d(64, n_classes, kernel_size=1)
        else:
            self.final = nn.Conv2d(64, n_classes, kernel_size=1)


    def forward(self, input):
        x0_0 = self.conv0_0(input)
        x1_0 = self.conv1_0(self.pool(x0_0)) #down
        x0_1 = self.conv0_1(torch.cat([x0_0, self.up1(x1_0)], 1)) #up and  cat

        x2_0 = self.conv2_0(self.pool(x1_0)) #down
        x1_1 = self.conv1_1(torch.cat([x1_0, self.up2(x2_0)], 1)) #up and  cat
        x0_2 = self.conv0_2(torch.cat([x0_0, x0_1, self.up1(x1_1)], 1))

        x3_0 = self.conv3_0(self.pool(x2_0))
        x2_1 = self.conv2_1(torch.cat([x2_0, self.up3(x3_0)], 1))
        x1_2 = self.conv1_2(torch.cat([x1_0, x1_1, self.up2(x2_1)], 1))
        x0_3 = self.conv0_3(torch.cat([x0_0, x0_1, x0_2, self.up1(x1_2)], 1))

        x4_0 = self.conv4_0(self.pool(x3_0))
        x3_1 = self.conv3_1(torch.cat([x3_0, self.up4(x4_0)], 1))
        x2_2 = self.conv2_2(torch.cat([x2_0, x2_1, self.up3(x3_1)], 1))
        x1_3 = self.conv1_3(torch.cat([x1_0, x1_1, x1_2, self.up2(x2_2)], 1))
        x0_4 = self.conv0_4(torch.cat([x0_0, x0_1, x0_2, x0_3, self.up1(x1_3)], 1))

        if self.deep_supervision:
            output1 = self.final1(x0_1)
            output2 = self.final2(x0_2)
            output3 = self.final3(x0_3)
            output4 = self.final4(x0_4)
            return [output1, output2, output3, output4]

        else:
            output = self.final(x0_4)
            return output

class Attention_block(nn.Module):
    """
    Attention Block
    """

    def __init__(self, F_g, F_l, F_int):
        super(Attention_block, self).__init__()

        self.W_g = nn.Sequential(
            nn.Conv2d(F_l, F_int, kernel_size=1, stride=1, padding=0, bias=True),
            nn.BatchNorm2d(F_int)
        )

        self.W_x = nn.Sequential(
            nn.Conv2d(F_g, F_int, kernel_size=1, stride=1, padding=0, bias=True),
            nn.BatchNorm2d(F_int)
        )

        self.psi = nn.Sequential(
            nn.Conv2d(F_int, 1, kernel_size=1, stride=1, padding=0, bias=True),
            nn.BatchNorm2d(1),
            nn.Sigmoid()
        )

        self.relu = nn.ReLU(inplace=True)

    def forward(self, g, x):
        g1 = self.W_g(g)
        x1 = self.W_x(x)
        psi = self.relu(g1 + x1)
        psi = self.psi(psi)
        out = x * psi
        return out

class upsample_block(nn.Module):
    def __init__(self, in_ch, out_ch):
        super(upsample_block, self).__init__()

        # self.up = nn.Sequential(
        #     # nn.Conv2d(ch_in,ch_out,3,1,1),
        #     # nn.BatchNorm2d(ch_out),
        #     # nn.ReLU(inplace=True ),
        #     nn.ConvTranspose2d(ch_in,ch_out,kernel_size=2,stride=2),
        #     nn.BatchNorm2d(ch_out),
        #     nn.ReLU(inplace=True )
        # )

        self.up = nn.Sequential(
              nn.Upsample(scale_factor=2),
              nn.Conv2d(in_ch, out_ch, kernel_size=3, stride=1, padding=1, bias=True),
              nn.BatchNorm2d(out_ch),
              nn.ReLU(inplace=True)
          )

    def forward(self, x):
        x = self.up(x)

        return x


class Practice(nn.Module):
    def __init__(self, n_channels, n_classes, deep_supervision=False):
        super().__init__()
        self.deep_supervision = deep_supervision
        self.pool = nn.MaxPool2d(2, 2)
        #self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)

        # self.up1=upsample_block(128,64)
        # self.up2=upsample_block(256,128)
        # self.up3=upsample_block(512,256)
        # self.up4=upsample_block(1024,512)
        self.up1=nn.ConvTranspose2d(128,64,kernel_size=2,stride=2)
        self.up2=nn.ConvTranspose2d(256,128,kernel_size=2,stride=2)
        self.up3=nn.ConvTranspose2d(512,256,kernel_size=2,stride=2)
        self.up4=nn.ConvTranspose2d(1024,512,kernel_size=2,stride=2)

        self.conv1=DoubleConv(64, 64)
        self.conv2=DoubleConv(128, 128)
        self.conv3=DoubleConv(256, 256)
        self.conv4=DoubleConv(512, 512)
        
        self.conv22=DoubleConv(256*4, 256)
        self.conv33=DoubleConv(128*5, 128)
        self.conv44=DoubleConv(64*6, 64)

        self.conv0_0 = DoubleConv(n_channels, 64)
        self.conv1_0 = DoubleConv(64, 128)
        self.conv2_0 = DoubleConv(128, 256)
        self.conv3_0 = DoubleConv(256, 512)
        self.conv4_0 = DoubleConv(512, 1024)

        self.conv0_1 = DoubleConv(64*2, 64)
        self.conv1_1 = DoubleConv(128*2, 128)
        self.conv2_1 = DoubleConv(256*2, 256 )
        self.conv3_1 = DoubleConv(512*2, 512)

        self.conv0_2 = DoubleConv(64*3,64)
        self.conv1_2 = DoubleConv(128*3, 128)
        self.conv2_2 = DoubleConv(256*3, 256)

        self.conv0_3 = DoubleConv(64*4, 64)
        self.conv1_3 = DoubleConv(128*4, 128)

        self.conv0_4 = DoubleConv(64*5, 64)

        self.att1 = Attention_block(F_g=512, F_l=512, F_int=256)
        self.att2 = Attention_block(F_g=256, F_l=256, F_int=128)
        self.att3 = Attention_block(F_g=128, F_l=128, F_int=64)
        self.att4 = Attention_block(F_g=64, F_l=64, F_int=32)

        if self.deep_supervision:
            self.final1 = nn.Conv2d(64, n_classes, kernel_size=1)
            self.final2 = nn.Conv2d(64, n_classes, kernel_size=1)
            self.final3 = nn.Conv2d(64, n_classes, kernel_size=1) 
            self.final4 = nn.Conv2d(64, n_classes, kernel_size=1)
        else:
            self.final = nn.Conv2d(64, n_classes, kernel_size=1)


    def forward(self, input):
        x0_0 = self.conv0_0(input)
        x1_0 = self.conv1_0(self.pool(x0_0)) #down
        x0_1 = self.conv0_1(torch.cat([x0_0, self.up1(x1_0)], 1)) #up and  cat

        x2_0 = self.conv2_0(self.pool(x1_0)) #down
        x1_1 = self.conv1_1(torch.cat([x1_0, self.up2(x2_0)], 1)) #up and  cat
        x0_2 = self.conv0_2(torch.cat([x0_0, x0_1, self.up1(x1_1)], 1))

        x3_0 = self.conv3_0(self.pool(x2_0)) #512
        x2_1 = self.conv2_1(torch.cat([x2_0, self.up3(x3_0)], 1)) #256
        x1_2 = self.conv1_2(torch.cat([x1_0, x1_1, self.up2(x2_1)], 1))
        x0_3 = self.conv0_3(torch.cat([x0_0, x0_1, x0_2, self.up1(x1_2)], 1))

        x4_0 = self.conv4_0(self.pool(x3_0))


        up_x4_0=self.up4(x4_0) #1024->512
        x3_0=self.att1(g=up_x4_0, x=x3_0) #512 512 => 512
        x3_1 = self.conv3_1(torch.cat([x3_0,up_x4_0], 1)) #512+512 -> 512



        up_x3_1=self.up3(x3_1) #512->256
        # x2=self.conv2_1(torch.cat([x2_0, x2_1],1)) #512->256
        # x2=self.att2(g=up_x3_1, x=x2) #256 ,256
        # x2_2 = self.conv2_1(torch.cat([x2,up_x3_1], 1)) # 256+256 ->256
       # x2_2 =self.conv3(x2_2)

        x2=self.att2(g=up_x3_1, x=x2_0) #256 ,256
        x2_2 = self.conv22(torch.cat([x2_0,x2,up_x3_1,x2_1], 1)) # 256+256+256+256 ->256



        up_x2_2=self.up2(x2_2) #256->128
        # x1=self.conv1_2(torch.cat([x1_0, x1_1, x1_2],1))
        # x1=self.att3(g=up_x2_2, x=x1) #128 128 
        # x1_3 = self.conv1_1(torch.cat([x1,up_x2_2], 1)) #128+128 ->128
     #   x1_3 =self.conv2(x1_3)

        x1=self.att3(g=up_x2_2, x=x1_0) #128 128 
        x1_3 = self.conv33(torch.cat([x1_0, x1_1,x1,up_x2_2,x1_2], 1)) #128+128 ->128



        up_x1_3=self.up1(x1_3) #128->64
        # x0=self.conv0_3(torch.cat([x0_0, x0_1, x0_2, x0_3],1))
        # x0=self.att4(g=up_x1_3, x=x0) #64 64
        # x0_4 = self.conv0_1(torch.cat([x0,up_x1_3], 1)) #64 =64 ->64
  #       x0_4 =self.conv1(x0_4)

        x0=self.att4(g=up_x1_3, x=x0_0) #64 64
        x0_4 = self.conv44(torch.cat([x0_0, x0_1, x0_2,x0,up_x1_3,x0_3], 1)) #64 =64 ->64



        if self.deep_supervision:
            output1 = self.final1(x0_1)
            output2 = self.final2(x0_2)
            output3 = self.final3(x0_3)
            output4 = self.final4(x0_4)
            return (output1+ output2+ output3+ output4)/4

        else:
            output = self.final(x0_4)
            return output

from __future__ import print_function, division
import torch.nn as nn
import torch.nn.functional as F
import torch.utils.data
import torch


class conv_block(nn.Module):
    """
    Convolution Block 
    """
    def __init__(self, in_ch, out_ch):
        super(conv_block, self).__init__()
        
        self.conv = nn.Sequential(
            nn.Conv2d(in_ch, out_ch, kernel_size=3, stride=1, padding=1, bias=True),
            nn.BatchNorm2d(out_ch),
            nn.ReLU(inplace=True),
            nn.Conv2d(out_ch, out_ch, kernel_size=3, stride=1, padding=1, bias=True),
            nn.BatchNorm2d(out_ch),
            nn.ReLU(inplace=True))

    def forward(self, x):

        x = self.conv(x)
        return x


class up_conv(nn.Module):
    """
    Up Convolution Block
    """
    def __init__(self, in_ch, out_ch):
        super(up_conv, self).__init__()
        self.up = nn.Sequential(
            nn.Upsample(scale_factor=2),
            nn.Conv2d(in_ch, out_ch, kernel_size=3, stride=1, padding=1, bias=True),
            nn.BatchNorm2d(out_ch),
            nn.ReLU(inplace=True)
        )

    def forward(self, x):
        x = self.up(x)
        return x
class AttU_Net(nn.Module):
    def __init__(self, img_ch=3, output_ch=1):
        super(AttU_Net, self).__init__()

        n1 = 64
        filters = [n1, n1 * 2, n1 * 4, n1 * 8, n1 * 16]

        self.Maxpool1 = nn.MaxPool2d(kernel_size=2, stride=2)
        self.Maxpool2 = nn.MaxPool2d(kernel_size=2, stride=2)
        self.Maxpool3 = nn.MaxPool2d(kernel_size=2, stride=2)
        self.Maxpool4 = nn.MaxPool2d(kernel_size=2, stride=2)

        self.Conv1 = conv_block(img_ch, filters[0])
        self.Conv2 = conv_block(filters[0], filters[1])
        self.Conv3 = conv_block(filters[1], filters[2])
        self.Conv4 = conv_block(filters[2], filters[3])
        self.Conv5 = conv_block(filters[3], filters[4])

        self.Up5 = up_conv(filters[4], filters[3])
        self.Att5 = Attention_block(F_g=filters[3], F_l=filters[3], F_int=filters[2])
        self.Up_conv5 = conv_block(filters[4], filters[3])

        self.Up4 = up_conv(filters[3], filters[2])
        self.Att4 = Attention_block(F_g=filters[2], F_l=filters[2], F_int=filters[1])
        self.Up_conv4 = conv_block(filters[3], filters[2])

        self.Up3 = up_conv(filters[2], filters[1])
        self.Att3 = Attention_block(F_g=filters[1], F_l=filters[1], F_int=filters[0])
        self.Up_conv3 = conv_block(filters[2], filters[1])

        self.Up2 = up_conv(filters[1], filters[0])
        self.Att2 = Attention_block(F_g=filters[0], F_l=filters[0], F_int=32)
        self.Up_conv2 = conv_block(filters[1], filters[0])

        self.Conv = nn.Conv2d(filters[0], output_ch, kernel_size=1, stride=1, padding=0)

        #self.active = torch.nn.Sigmoid()


    def forward(self, x):

        e1 = self.Conv1(x)

        e2 = self.Maxpool1(e1)
        e2 = self.Conv2(e2)

        e3 = self.Maxpool2(e2)
        e3 = self.Conv3(e3)

        e4 = self.Maxpool3(e3)
        e4 = self.Conv4(e4)

        e5 = self.Maxpool4(e4)
        e5 = self.Conv5(e5)

        #print(x5.shape)
        d5 = self.Up5(e5)
        #print(d5.shape)
        x4 = self.Att5(g=d5, x=e4)
        d5 = torch.cat((x4, d5), dim=1)
        d5 = self.Up_conv5(d5)

        d4 = self.Up4(d5)
        x3 = self.Att4(g=d4, x=e3)
        d4 = torch.cat((x3, d4), dim=1)
        d4 = self.Up_conv4(d4)

        d3 = self.Up3(d4)
        x2 = self.Att3(g=d3, x=e2)
        d3 = torch.cat((x2, d3), dim=1)
        d3 = self.Up_conv3(d3)

        d2 = self.Up2(d3)
        x1 = self.Att2(g=d2, x=e1)
        d2 = torch.cat((x1, d2), dim=1)
        d2 = self.Up_conv2(d2)

        out = self.Conv(d2)

      #  out = self.active(out)

        return out

"""#Training"""

#loss 평균값 도출하기 위해  

class AverageMeter(object):
  def __init__(self):
    self.reset()
  def reset(self):
    self.val, self.avg, self.sum, self.count = 0, 0, 0, 0
  def update(self, val, n=1):
    self.val = val
    self.sum += val * n
    self.count += n
    self.avg = self.sum / self.count

#train 진행한 네트워크 저장하고 불러오기

def save(checkpoint_dir,network,optim,epoch):
  if not os.path.exists(checkpoint_dir):
    os.makedirs(checkpoint_dir)
  
  torch.save({'net':network.state_dict(),
              'optim':optim.state_dict()},
              '%s/model_epoch%d.pth'%(checkpoint_dir,epoch))
  
  
def load(checkpoint_dir,network,optim):
  if not os.path.exists(checkpoint_dir):
    epoch=1
    return network,optim,epoch
  
  checkpoint_list=os.listdir(checkpoint_dir)
  checkpoint_list.sort(key=lambda f : int(''.join(filter(str.isdigit,f))))

  dict_model=torch.load('%s/%s'%(checkpoint_dir,checkpoint_list[-1]))

  network.load_state_dict(dict_model['net'])
  optim.load_state_dict(dict_model['optim'])
  epoch=int(checkpoint_list[-1].split('epoch')[1].split('.pth')[0])

  return network,optim,epoch

import os
import sys 
import math
import torch
import numpy as np
import cv2

def torchPSNR(tar_img, prd_img):
    # imdff= tar_img.cpu().detach().numpy()-prd_img.cpu().detach().numpy()
    # rmse = math.sqrt(np.mean((imdff)**2))
    # ps = 20 * math.log10(1/rmse)
    imdff = torch.clamp(prd_img,0,1) - torch.clamp(tar_img,0,1)
    rmse = (imdff**2).mean().sqrt()
    ps = 20*torch.log10(1/rmse)
    return ps

pip install IQA_pytorch

from IQA_pytorch import SSIM, utils
from PIL import Image
import torch
from torch.utils.tensorboard import SummaryWriter
import numpy as np
writer = SummaryWriter('scalar/')

# function for training
def train(model, train_dataloader,criterion, optimizer, epoch):
    model.train()
    losses = AverageMeter()
    psnr_val=[]
    ssim_val=[]


    for i,data in enumerate(tqdm.tqdm(train_dataloader)):
        if use_cuda:
          l=data["l"].to('cuda')
          ab=data['ab'].to('cuda')
          hint=data['hint'].to('cuda')
        
        gt_image=torch.cat((l,ab),dim=1)
        hint_image=torch.cat((l,hint),dim=1)

        predict=model(hint_image)

        optimizer.zero_grad()

        

        loss=criterion(predict,gt_image)
        losses.update(loss.item(),hint_image.size(0))


        loss.backward()
        optimizer.step()

        psnr_val.append(torchPSNR(predict,gt_image))

        ref = utils.prepare_image(cv2.cvtColor(tensor2im(gt_image),cv2.COLOR_LAB2RGB)).to('cuda')
        dist = utils.prepare_image(cv2.cvtColor(tensor2im(predict),cv2.COLOR_LAB2RGB)).to('cuda')       
        ssim = SSIM(channels=3)
        ssim_val.append(ssim(dist, ref, as_loss=False).item())


        if i%100==0:
          print('\nEpoch : [{}] {}/{} \n Training loss : {}'.format(epoch,i,len(train_dataloader),losses.avg))

    psnr=sum(psnr_val)/len(psnr_val)
    ssim=sum(ssim_val)/len(ssim_val)

    print('\nFinished training epoch {}'.format(epoch))
    print('Total training loss : {} / psnr : {:.4f} / ssim : {:.4f}'.format(losses.avg,psnr,ssim))
    save(ckpt_dir, model, optimizer, epoch)

    return losses.avg,psnr,ssim




# Function for validatation
def validation(model, validation_dataloader,criterion):
    model.eval()
    losses = AverageMeter()
    psnr_val=[]
    ssim_val=[]

    with torch.no_grad():
        for i,data in enumerate(tqdm.tqdm(validation_dataloader)):
          if use_cuda:
            l=data["l"].to('cuda') 
            ab=data['ab'].to('cuda')
            hint=data['hint'].to('cuda')
          
          gt_image=torch.cat((l,ab),dim=1)
          hint_image=torch.cat((l,hint),dim=1)

          predict=model(hint_image)
          loss=criterion(predict,gt_image)
          losses.update(loss.item(),hint_image.size(0))

          gt_np=tensor2im(gt_image)
          gt_bgr=cv2.cvtColor(gt_np,cv2.COLOR_LAB2BGR)
          predict_np=tensor2im(predict)
          predict_bgr=cv2.cvtColor(predict_np,cv2.COLOR_LAB2BGR)
          cv2.imwrite(data_dir+"/output_"+str(i)+".png", predict_bgr)
          

          psnr_val.append(torchPSNR(predict,gt_image))

          ref = utils.prepare_image(cv2.cvtColor(gt_bgr,cv2.COLOR_BGR2RGB)).to('cuda')
          dist = utils.prepare_image(cv2.cvtColor(predict_bgr,cv2.COLOR_BGR2RGB)).to('cuda')       
          ssim = SSIM(channels=3)
          ssim_val.append(ssim(dist, ref, as_loss=False).item())




          if i%100==0:
            gt_np=tensor2im(gt_image)
            gt_bgr=cv2.cvtColor(gt_np,cv2.COLOR_LAB2BGR)
            hint_np=tensor2im(hint_image)
            hint_bgr=cv2.cvtColor(hint_np,cv2.COLOR_LAB2BGR)   
            cv2_imshow(hint_bgr)         
            cv2_imshow(gt_bgr)
            cv2_imshow(predict_bgr)
                

        psnr=sum(psnr_val)/len(psnr_val)
        ssim=sum(ssim_val)/len(ssim_val)

    return losses.avg,psnr,ssim

import torch.optim as optim
import time
import numpy as np
import torch.nn as nn
import torch

# data_dir = '/content/drive/MyDrive/UNet/output'
# ckpt_dir = '/content/drive/MyDrive/UNet/checkpoint'


# data_dir = '/content/drive/MyDrive/NestedUNet/output'
# ckpt_dir = '/content/drive/MyDrive/NestedUNet/checkpoint'


# data_dir = '/content/drive/MyDrive/Att_UNet/output'
# ckpt_dir = '/content/drive/MyDrive/Att_UNet/checkpoint'


# data_dir = '/content/drive/MyDrive/Dense_UNet/output'
# ckpt_dir = '/content/drive/MyDrive/Dense_UNet/checkpoint'


# data_dir = '/content/drive/MyDrive/Dense_Att_UNet/output'
# ckpt_dir = '/content/drive/MyDrive/Dense_Att_UNet/checkpoint'

data_dir = '/content/drive/MyDrive/PRACTICE_NET/output'
ckpt_dir = '/content/drive/MyDrive/PRACTICE_NET/checkpoint'

if not os.path.exists(data_dir):
    os.makedirs(data_dir)

# root_path="/content/drive/MyDrive/UNet/"
# root_path="/content/drive/MyDrive/NestedUNet/"
# root_path="/content/drive/MyDrive/Nested_Att_UNet/"
# root_path='/content/drive/MyDrive/Dense_UNet/'
# root_path='/content/drive/MyDrive/Dense_Att_UNet/'
root_path='/content/drive/MyDrive/PRACTICE_NET/'
use_cuda=True


#하이퍼 파라미터
batch_size=4
learning_rate=1e-3
EPOCH=120

#모델 불러오기
# model=Unet(n_channels=3, n_classes=3, bilinear=False)
#model=Nested_Unet(n_channels=3, n_classes=3, deep_supervision=False)
# model=Nested_Attention_Unet(n_channels=3, n_classes=3, deep_supervision=False)
# model=AttU_Net(3,3)
# model=Dense_Unet(3,3)

model=Practice(3,3)

#optimizer 및 loss function
optimizer = optim.Adam(model.parameters(), lr=learning_rate) #, eps=1e-8, weight_decay=1e-8
criterion =  nn.L1Loss()

if use_cuda:
  model=model.cuda()
  criterion=criterion.cuda()



#Training
start_epoch=1
model, optimizer, start_epoch = load(ckpt_dir,model,optimizer)

best=0

for epoch in range(start_epoch, EPOCH+1):
  epoch_start_time=time.time()
  train_loss, train_PSNR,train_SSIM = train(model, train_dataloader,criterion, optimizer, epoch)
  with torch.no_grad():
    val_loss, val_PSNR,val_SSIM = validation(model, valid_dataloader,criterion)


  if val_PSNR > best:
     best = val_PSNR
     torch.save({'net':model.state_dict(),
              'optim':optimizer.state_dict()},
              root_path+'best_model_epoch%d_%lf.pth'%(epoch,best))
  print("\n------------------------------------------------------------------")
  print("Epoch: {}\tTime: {:.4f}\tTrain Loss: {:.4f}\tTrain PSNR {:.4f}\tTrain SSIM {:.4f}".format(epoch, time.time()-epoch_start_time, train_loss, train_PSNR,train_SSIM))
  print("------------------------------------------------------------------")
  print('\n[{}] Validation Loss : {:.4f}, Validation PSNR : {:.4f}% ,Validation SSIM : {:.4f}%'.format(epoch, val_loss, val_PSNR,val_SSIM))

  


  writer.add_scalar('Loss/train', train_loss, epoch)
  writer.add_scalar('Loss/validation', val_loss, epoch)
  writer.add_scalar('PSNR/train', train_PSNR, epoch)
  writer.add_scalar('PSNR/validation', val_PSNR, epoch)
  writer.add_scalar('SSIM/train', train_SSIM, epoch)
  writer.add_scalar('SSIM/validation', val_SSIM, epoch)


writer.close()

pip install optuna

import random
import torch.optim as optim
import time
import numpy as np
import torch.nn as nn
import torch

def train_with_optuna(trial): 
  cfg = { 
         'device' : "cuda" if torch.cuda.is_available() else "cpu",
         'seed' : 0, 
         'lr' : trial.suggest_uniform("lr", 1e-4, 1e-3),
         'optimizer': trial.suggest_categorical('optimizer',[optim.SGD, optim.Adam,optim.RMSprop]),
         'criterion': trial.suggest_categorical('criterion',["1","2"])
         } 

  if cfg['criterion']=='1':
    criterion=nn.L1Loss()
  else:
    criterion=nn.MSELoss()

 
         
         
  torch.manual_seed(cfg['seed']) 

  model = Practice(3,3)
  optimizer = optim.Adam(model.parameters(), lr=cfg['lr']) 
  #criterion =  cfg['criterion']

  if use_cuda:
    model=model.cuda()
    criterion=criterion.cuda()


  best=0

  
  for epoch in range(0,5):
    train_loss, train_PSNR,train_SSIM = train(model, train_dataloader,criterion, optimizer, epoch)
    with torch.no_grad():
      val_loss, val_PSNR,val_SSIM = validation(model, valid_dataloader,criterion)


    if val_PSNR > best:
      best = val_PSNR


  return best

pip install joblib

import optuna 
import joblib

study = optuna.create_study(sampler=optuna.samplers.TPESampler(), direction='maximize')

# study.optimize(train_with_optuna, n_trials=20)

joblib.dump(study, '/content/drive/MyDrive/_optuna.pkl')


study = joblib.load('/content/drive/MyDrive/_optuna.pkl') 
df = study.trials_dataframe().drop(['state','datetime_start','datetime_complete'], axis=1) 
df=df.sort_values('value',ascending=False)

df.loc[df.params_criterion=='1','params_criterion']='L1Loss'

df.loc[df.params_criterion=='2','params_criterion']='MSELoss'
df.head(5)

df.head(10)

df.head(10)

"""#Test"""

import os
import zipfile
import tqdm

file_name = "results.zip"
zip_path = os.path.join('/content/drive/MyDrive/results.zip')

!cp "{zip_path}" .
!unzip -q "{file_name}"
!rm "{file_name}"

test_dir = '/content/drive/MyDrive/PRACTICE_NET/test_result'


if not os.path.exists(test_dir):
    os.makedirs(test_dir)

# model=Unet(n_channels=3, n_classes=3, bilinear=False)
# model.load_state_dict(torch.load("/content/drive/MyDrive/UNet/checkpoint/model_epoch40.pth")['net'])


# model=Nested_Unet(n_channels=3, n_classes=3, deep_supervision=False)
# model.load_state_dict(torch.load("/content/drive/MyDrive/NestedUNet/best_model_epoch80_37.406292.pth")['net'])

model=Practice(3,3)
model.load_state_dict(torch.load("/content/drive/MyDrive/PRACTICE_NET/checkpoint/model_epoch79.pth")['net'])
model.cuda()


# Function for validatation
def test(model, test_dataloader):
    model.eval()
    psnr_test=[]
    ssim_test=[]

    with torch.no_grad():
        for i,data in enumerate(tqdm.tqdm(test_dataloader)):
          if use_cuda:
            l=data["l"].to('cuda')
            ab=data['ab'].to('cuda')
            hint=data['hint'].to('cuda')
          
          gt_image=torch.cat((l,ab),dim=1)
          hint_image=torch.cat((l,hint),dim=1)


          predict=model(hint_image)
          psnr_test.append(torchPSNR(predict,gt_image))

          predict_np=tensor2im(predict)
          predict_bgr=cv2.cvtColor(predict_np,cv2.COLOR_LAB2BGR)
          #cv2.imwrite(test_dir+"/output_"+str(i)+".png", predict_bgr)

          gt_np=tensor2im(gt_image)
          gt_bgr=cv2.cvtColor(gt_np,cv2.COLOR_LAB2BGR)


          ref = utils.prepare_image(cv2.cvtColor(gt_bgr,cv2.COLOR_BGR2RGB)).to('cuda')
          dist = utils.prepare_image(cv2.cvtColor(predict_bgr,cv2.COLOR_BGR2RGB)).to('cuda')       
          ssim = SSIM(channels=3)
          ssim_test.append(ssim(dist, ref, as_loss=False).item())


          # if i%100==0:
          hint_np=tensor2im(hint_image)
          hint_bgr=cv2.cvtColor(hint_np,cv2.COLOR_LAB2BGR)   
          cv2_imshow(hint_bgr)         
          cv2_imshow(gt_bgr)
          cv2_imshow(predict_bgr)
          print("psnr : ",torchPSNR(predict,gt_image))



    psnr_=sum(psnr_test)/len(psnr_test)
    ssim_=sum(ssim_test)/len(ssim_test)

    return psnr_,ssim_


psnr,ssim=test(model, test_dataloader)
print("psnr : %lf  ssim : %lf"%(psnr,ssim))

pip install tensorboard

from torch.utils.tensorboard import SummaryWriter
import numpy as np
writer = SummaryWriter('scalar/')


for n_iter in range(100):
    writer.add_scalar('Loss/train', np.random.random(), n_iter)
    writer.add_scalar('Loss/test', np.random.random(), n_iter)
    writer.add_scalar('Accuracy/train', np.random.random(), n_iter)
    writer.add_scalar('Accuracy/test', np.random.random(), n_iter)

writer.close()

# Commented out IPython magic to ensure Python compatibility.
# %reload_ext tensorboard
# %tensorboard --logdir ./scalar

